# 项目总览与快速开始

## 1. 项目定位

`my-yolo-test` 是一个“动作识别 + 手与物体接触检测”项目，核心目标包括：

- 基于人体姿态关键点识别基础动作（如挥手、点头、摇头）；
- 检测“手是否接触了目标物体”；
- 在真实场景（遮挡、抖动、同物体重复检测、类别冲突）中保持较高鲁棒性。

项目技术栈：

- `Ultralytics YOLO`：姿态与分割检测；
- `MediaPipe Hand Landmarker`：手部关键点；
- `OpenCV`：视频流读取与可视化；
- `uv`：Python 依赖与运行管理。

## 2. 环境要求

- Python：`>=3.10`
- 推荐系统：Windows / Linux（本项目在 Windows + PowerShell 下长期开发）
- 摄像头：建议固定焦距 + 稳定光照，便于关键点与分割稳定

## 3. uv 管理方式（必须）

本项目由 `uv` 管理，推荐仅使用 `uv` 执行命令。

常用命令：

```bash
# 安装依赖（首次）
uv sync

# 运行主程序
uv run python -m src.app --help

# 运行任意工具脚本
uv run python tools/train/segment/train_segment_objects.py --help
```

为什么坚持 `uv`：

- 依赖解析快，虚拟环境一致性好；
- 命令可复现，不容易出现“本机能跑、别人不能跑”的问题；
- 与 `pyproject.toml`、`uv.lock` 配合，版本可追踪。

## 4. 最小可运行示例

```bash
uv run python -m src.app \
  --pose-model models/yolo26n-pose.pt \
  --det-model models/yolo26n-seg.pt \
  --det-whitelist-config training/configs/segment_data.yaml \
  --det-whitelist-mode override \
  --debug
```

说明：

- `--debug` 会统一打开调试叠加（FPS、目标计数、ROI 数、各阶段耗时等）；
- `--det-whitelist-config` 会从训练配置文件自动导入类别白名单；
- `--det-whitelist-mode override` 表示完全采用 YAML 中的 `names`。
- 目标可视化默认优先显示分割 `mask` 轮廓，若只想显示 `bbox` 可加 `--no-draw-mask-edges`。

## 5. 常见使用路径

### 5.1 只跑推理

1. 准备模型权重；
2. 直接运行 `src.app`；
3. 根据画面表现调 `--hand-roi-*`、`--obj-*`、`--contact-*` 参数。

### 5.2 训练 + 推理联动

1. 准备并转换数据（分割任务用 COCO JSON -> YOLO Seg）；
2. 训练分割模型；
3. 推理读取同一份 `data.yaml` 的 `names` 作为白名单，避免类别不一致。

详见：

- `docs/06_训练全流程指南.md`
- `docs/07_数据集格式规范.md`

## 6. 核心机制概览

1. `ROI 机制`：项目通过手部 ROI 放大来提升小目标识别概率；
2. `ROI 批量推理`：多个 ROI 会合并为单次批处理，降低重复推理开销；
3. `小物体聚焦`：ROI 有最大尺寸约束，并过滤 ROI 内“过大候选”；
4. `时序稳定`：手关键点与目标检测都做了时序稳定，并在可视化层抑制旧位置残影；
5. `unknown 回退`：ROI 内无已知类别时可按需开启轮廓回退补 `unknown` mask（默认关闭）；
6. `去重与冲突抑制`：同物体重复框、同物体跨类别冲突都有专门后处理，且时序输出后会再做一次兜底去重。

进一步阅读：

- `docs/03_运行时管线与核心算法.md`
- `docs/08_调参与排障手册.md`
